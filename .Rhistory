color = "red", linetype = "dashed") +  # Reference line
labs(title = "Actual vs Predicted Ozone",
x = "Predicted Ozone",
y = "Actual Ozone") +
theme_minimal()
#Add a red line to show where the actual and predicted values are equal This can be done by plotting a 1:1 line (e.g. intercept 0, slope 1) with geom_abline(intercept = 0, slope = 1, color = "red")
#Add a subtitle to the plot showing the correlation between the actual and predicted values are equal This can be done by plotting a 1:1 line (e.g. intercept 0, slope 1) with
#paste("Correlation:", round(cor(a$Ozone, a$.fitted),2)) assuming your augmented data object is called a
correlation_value <- round(cor(augmented_covid$Ozone, augmented_covid$.fitted), 2)
ggplot(augmented_covid, aes(x = .fitted, y = Ozone)) +
geom_point(color = "blue") +             # Actual vs predicted points
geom_abline(intercept = 0, slope = 1,   # Line y = x for comparison
color = "red", linetype = "dashed") +  # Reference line
labs(title = "Actual vs Predicted Ozone",
subtitle = paste("Correlation:", correlation_value),
x = "Predicted Ozone",
y = "Actual Ozone") +
theme_minimal()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
install.packages("visdat")
library(visdat)
is there a relationship between car weight and miles per gallon in the mtcars dataset?”.
explortory data analysis
Exploratory data Analysis (2016) Peng
knitr::opts_chunk$set(echo = TRUE)
penguins<-palmerpenguins::penguins_raw
View(penguins)
str(penguins)
typeof(penguins)
class(penguins)
mode(penguins)
sapply(penguins, is.numeric)
Be alert to structure and possible data problems or anomalies.
str(penguins)
library(visdat)
summary(penguins)
table(penguin)
table(penguins)
vis_dat(penguins)
range(penguins$`Sample Number`)
range(penguins$`Sample Number`,penguins$`Culmen Length (mm)`)
range(penguins$`Sample Number`)
range(penguins$`Culmen Length (mm)`)
range(penguins$`Culmen Length (mm)`)
range(penguins$`Culmen Depth (mm)`)
penguins %>% drop_na %>%range(penguins$`Culmen Length (mm)`)
penguins %>% na_drop %>%range(penguins$`Culmen Length (mm)`)
penguins %>% na.rm= TRUE %>%range(penguins$`Culmen Length (mm)`)
penguins %>% rm.na= TRUE %>%range(penguins$`Culmen Length (mm)`)
penguins %>% drop_na() %>% range(penguins$`Culmen Length (mm)`)
library(tidyr)
library(tidyverse)
penguins %>% drop_na() %>% range(penguins$`Culmen Length (mm)`)
penguins %>%
drop_na() %>%
range(`Culmen Length (mm)`)
penguins %>%
drop_na() %>%
range(,`Culmen Length (mm)`)
penguins <-penguins %>%
drop_na()
range(penguins$`Culmen Length (mm)`)
range(penguins$`Culmen Depth (mm)`)
range(penguins$`Flipper Length (mm)`)
range(penguins$`Body Mass (g)`)
range(penguins$`Delta 15 N (o/oo)`)
range(penguins$`Delta 13 C (o/oo)`)
#mean
skimr::skim(penguins$`Sample Number`)
skimr::skim(penguins$`Culmen Length (mm)`)
skimr::skim(penguins$`Culmen Depth (mm)`)
skimr::skim(penguins$`Flipper Length (mm)`)
skimr::skim(penguins$`Body Mass (g)`)
skimr::skim(penguins$`Delta 15 N (o/oo)`)
skimr::skim(penguins$`Delta 13 C (o/oo)`)
#mean
summary(penguins$`Sample Number`)
summary(penguins$`Culmen Length (mm)`)
summary(penguins$`Culmen Depth (mm)`)
summary(penguins$`Flipper Length (mm)`)
summary(penguins$`Body Mass (g)`)
summary(penguins$`Delta 15 N (o/oo)`)
summary(penguins$`Delta 13 C (o/oo)`)
class(penguins)
penguins %>%
count(species)
penguins %>%
count(Species)
penguins %>%
count(Island)
penguins %>%
count(Sex)
penguins %>%
count(Species)
penguins %>%
count(Island)
penguins %>%
count(Island)
penguins %>%
count(Sex)
table(penguins$flipper_length_mm)
table(penguins$'Flipper Length')
table(penguins$`Flipper Length (mm)`)
# Table the binary variable: sex
table(penguins$Sex)
# View the first few rows (top of the dataset)
head(penguins)
# View the last few rows (bottom of the dataset)
tail(penguins)
skimr::skim(penguins)
# Summary statistics of body mass by island
penguins_summary <- penguins %>%
group_by(island) %>%
summarise(
mean_weight = mean(body_mass_g, na.rm = TRUE),
median_weight = median(body_mass_g, na.rm = TRUE),
sd_weight = sd(body_mass_g, na.rm = TRUE),
min_weight = min(body_mass_g, na.rm = TRUE),
max_weight = max(body_mass_g, na.rm = TRUE)
)
# Summary statistics of body mass by island
penguins_summary <- penguins %>%
group_by(Island) %>%
summarise(
mean_weight = mean(`Body Mass (g)`, na.rm = TRUE),
median_weight = median(`Body Mass (g)`, na.rm = TRUE),
sd_weight = sd(`Body Mass (g)`, na.rm = TRUE),
min_weight = min(`Body Mass (g)`, na.rm = TRUE),
max_weight = max(`Body Mass (g)`, na.rm = TRUE)
)
# View summary statistics
print(penguins_summary)
ggscatter(penguins, x = Island, y = `Body Mass (g)`,
color = "Island", add = "reg.line", conf.int = TRUE) +
labs(title = "Island vs. Penguin Body Mss (g)",
x = "Island", y = "Body size(g)")
library(ggpubr)
ggscatter(penguins, x = Island, y = `Body Mass (g)`,
color = "Island", add = "reg.line", conf.int = TRUE) +
labs(title = "Island vs. Penguin Body Mss (g)",
x = "Island", y = "Body size(g)")
ggscatter(penguins, x = Island, y = `Body Mass (g)`,
color = Island, add = "reg.line", conf.int = TRUE) +
labs(title = "Island vs. Penguin Body Mss (g)",
x = "Island", y = "Body size(g)")
ggscatter(penguins, x = Island, y = `Body Mass (g)`,
color = Island, add = "reg.line", conf.int = TRUE) +
labs(title = "Island vs. Penguin Body Mss (g)",
x = "Island", y = "Body size(g)")
ggscatter(penguins, x = Species, y = `Body Mass (g)`,
color = Island, add = "reg.line", conf.int = TRUE) +
labs(title = "Island vs. Penguin Body Mss (g)",
x = "Island", y = "Body size(g)")
ggscatter(penguins, x = Species, y = `Body Mass (g)`,
color="Island", add = "reg.line", conf.int = TRUE) +
labs(title = "Island vs. Penguin Body Mss (g)",
x = "Island", y = "Body size(g)")
ggscatter(penguins, x = "Species", y = "`Body Mass (g)`",
color="Island", add = "reg.line", conf.int = TRUE) +
labs(title = "Island vs. Penguin Body Mss (g)",
x = "Island", y = "Body size(g)")
ggscatter(penguins, x = "Island", y = "Body Mass (g)",
color="Island", add = "reg.line", conf.int = TRUE) +
labs(title = "Island vs. Penguin Body Mss (g)",
x = "Island", y = "Body size(g)")
ggscatter(penguins, x = "", y = "Body Mass (g)",
color = "Island", add = "reg.line", ellipse = TRUE, conf.int = TRUE) +
labs(title = "Island vs. Body Mass (g)",
x = "", y = "Body Mass (g)") +
facet_wrap(~Island)
ggscatter(penguins, x = "", y = "`Body Mass (g)`",
color = "Island", add = "reg.line", ellipse = TRUE, conf.int = TRUE) +
labs(title = "Island vs. Body Mass (g)",
x = "", y = "Body Mass (g)") +
facet_wrap(~Island)
ggscatter(penguins, x = "", y = `Body Mass (g)`,
color = "Island", add = "reg.line", ellipse = TRUE, conf.int = TRUE) +
labs(title = "Island vs. Body Mass (g)",
x = "", y = "Body Mass (g)") +
facet_wrap(~Island)
ggplot(penguins, aes(x = `Body Mass (g)`, fill = Island)) +
geom_histogram(binwidth = 100, position = "dodge", alpha = 0.7) +
labs(title = "Body Mass Distribution by Island",
x = "Body Mass (grams)", y = "Frequency") +
facet_wrap(~island) +
theme_minimal()
ggplot(penguins, aes(x = `Body Mass (g)`, fill = Island)) +
geom_histogram(binwidth = 100, position = "dodge", alpha = 0.7) +
labs(title = "Body Mass Distribution by Island",
x = "Body Mass (grams)", y = "Frequency") +
facet_wrap(~Island) +
theme_minimal()
summary(penguins)
table(penguins)
summary(penguins)
vis_dat(penguins)
penguins <-penguins %>%
drop_na()
#range
range(penguins$`Sample Number`)
range(penguins$`Culmen Length (mm)`)
range(penguins$`Culmen Depth (mm)`)
range(penguins$`Flipper Length (mm)`)
range(penguins$`Body Mass (g)`)
range(penguins$`Delta 15 N (o/oo)`)
range(penguins$`Delta 13 C (o/oo)`)
airquality<-airquality
str(airquality)
summary(airquality)
# Perform Shapiro-Wilk normality test
shapiro.test(airquality$Ozone)
# Perform Shapiro-Wilk normality test
shapiro.test(airquality$Ozone)
shapiro.test(airquality$Temp)
shapiro.test(airquality$Solar.R)
shapiro.test(airquality$Wind)
#remove na values
airquality_seasons <- airquality %>%
drop_na()
View(airquality)
# Create a new column 'Season' based on the 'Month' column
airquality_seasons <- airquality_seasons %>%
mutate(season = case_when(
Month == 11 | Month == 12 | Month == 1 ~ "Winter",
Month == 2 | Month == 3 | Month == 4 ~ "Spring",
Month == 5 | Month == 6 | Month == 7 ~ "Summer",
Month == 8 | Month == 9 | Month == 10 ~ "Fall",
TRUE ~ "Unknown"  # Handling any missing values (NA) if necessary
))
View(airquality_seasons)
# Count the number of observations for each season
table(airquality_seasons$season)
airquality_seasons <- airquality
airquality_seasons <- airquality_seasons %>%
mutate(season = case_when(
Month == 11 | Month == 12 | Month == 1 ~ "Winter",
Month == 2 | Month == 3 | Month == 4 ~ "Spring",
Month == 5 | Month == 6 | Month == 7 ~ "Summer",
Month == 8 | Month == 9 | Month == 10 ~ "Fall",
TRUE ~ "Unknown"  # Handling any missing values (NA) if necessary
))
# Count the number of observations for each season
table(airquality_seasons$season)
library(recipes)
#  recipe to normalize the variables Temp, Solar.R, and Wind
recipe_obj <- recipe(~ Temp + Solar.R + Wind + season, data = airquality_seasons) %>%
# Impute missing values with the mean for numeric columns
step_meanimpute(all_numeric(), -all_outcomes()) %>%
# Normalize the numeric predictors
step_normalize(all_numeric(), -all_outcomes())
#  recipe to normalize the variables Temp, Solar.R, and Wind
recipe_normalize <- recipe(~ Temp + Solar.R + Wind + season, data = airquality_seasons) %>%
# Impute missing values with the mean for numeric columns
step_impute_mean(all_numeric(), -all_outcomes()) %>%
# Normalize the numeric predictors
step_normalize(all_numeric(), -all_outcomes())
# Preview the recipe
recipe_normalize
# Prepare the recipe (estimate means and scaling parameters)
recipe_prep <- prep(recipe_normalize, training = airquality_seasons)
View(recipe_prep)
# Prepare the recipe (estimate means and scaling parameters)
recipe_prep <- prep(recipe_normalize, training = airquality_seasons)
# Apply the transformations to the data (bake)
airquality_processed <- bake(recipe_prep, new_data = airquality_seasons)
# View the processed dataset
head(airquality_processed)
lm(Ozone ~ ., data = airquality_seasons) %>%
summary()
# View the su
library(broom)
# broom::augment() function can be used to add the fitted values and residuals to your data
# Augment the data with fitted values and residuals
augmented_data <- augment(lm_model, data = airquality_seasons)
# Fit the linear regression model. lm() function to fit the model, and the . notation to include all predictor variables automatically.
airquality_lm<-lm(Ozone ~ ., data = airquality_seasons) %>%
summary()
# Fit the linear regression model. lm() function to fit the model, and the . notation to include all predictor variables automatically.
airquality_lm<-lm(Ozone ~ ., data = airquality_seasons) %>%
summary()
# broom::augment() function can be used to add the fitted values and residuals to your data
# Augment the data with fitted values and residuals
augmented_data <- augment(lm_model, data = airquality_seasons)
# broom::augment() function can be used to add the fitted values and residuals to your data
# Augment the data with fitted values and residuals
augmented_data <- augment(airquality_lm, data = airquality_seasons)
# Fit the linear regression model. lm() function to fit the model, and the . notation to include all predictor variables automatically.
airquality_lm<-lm(Ozone ~ ., data = airquality_seasons) %>%
summary()
View(airquality_lm)
# Fit the linear regression model. lm() function to fit the model, and the . notation to include all predictor variables automatically.
airquality_lm <- lm(Ozone ~ ., data = airquality_seasons) %>%
summary()
summary(airquality_lm)
# Augment the data with fitted values and residuals
augmented_data <- augment(airquality_lm, data = airquality_seasons)
install.packages("broom")
# Fit the linear regression model. lm() function to fit the model, and the . notation to include all predictor variables automatically.
airquality_lm <- lm(Ozone ~ ., data = airquality_seasons)
summary(airquality_lm)
# broom::augment() function can be used to add the fitted values and residuals to your data
# Augment the data with fitted values and residuals
augmented_data <- augment(airquality_lm, data = airquality_seasons)
sum(is.na(airquality_with_seasons))
sum(is.na(airquality_seasons))
# broom::augment() function can be used to add the fitted values and residuals to your data
airquality_nona <- na.omit(airquality_seasons)
# Augment the data with fitted values and residuals
augmented_data <- augment(airquality_lm, data = airquality_seasons)
# broom::augment() function can be used to add the fitted values and residuals to your data
#remove the na values
airquality_nona <- na.omit(airquality_seasons)
# Augment the data with fitted values and residuals
augmented_data <- augment(airquality_lm, data = airquality_nona)
# View the first few rows of the augmented data
head(augmented_data)
# Histogram of residuals
hist_plot <- ggplot(augmented_data, aes(x = .resid)) +
geom_histogram(bins = 30, fill = "skyblue", color = "black") +
labs(title = "Histogram of Residuals", x = "Residuals", y = "Frequency") +
theme_minimal()
# Q-Q plot of residuals
qq_plot <- ggplot(augmented_data, aes(sample = .resid)) +
geom_qq() +
geom_qq_line() +
labs(title = "Q-Q Plot of Residuals") +
theme_minimal()
library(gridExtra)
# Use ggarrange to combine the two plots into one image
grid.arrange(hist_plot, qq_plot, ncol = 2)
# Scatter plot of Actual vs. Predicted Ozone
ggscatter(augmented_data, x = "Ozone", y = ".fitted",
add = "reg.line", conf.int = TRUE,
cor.coef = TRUE, cor.method = "spearman",
ellipse = TRUE) +
labs(title = "Actual vs. Predicted Ozone") +
theme_minimal()
#Load tidy models and penguins data set
library(tidymodels)
penguins<-library(palmerpenguins)
penguins<-palmerpenguins::
penguins<-palmerpenguins
penguins<- palmerpenguins
#Load tidy models and penguins data set
library(tidymodels)
install.packages("tidymodels")
#Load tidy models and penguins data set
library(tidymodels)
penguins=palmerpenguins
library(palmerpenguins)
library(palmerpenguins)
penguins<-palmerpenguins
install.packages("palmerpenguins")
library(tidymodels)
## 1. Load tidy models and penguins data set.
#install penguins package if don't already have-->install.packages("palmerpenguins")
install.packages("tidymodels")
library(tidymodels)
library(tidymodels)
install.packages("cli")
library(tidymodels)
library(tidymodels)
library(cli)
install.packages(c("cli", "clock", "curl", "doFuture", "gert", "ijtiff", "imager", "jpeg", "knitr", "mime", "nloptr", "officer", "parallelly", "Rdpack", "recipes", "sparsevctrs", "tzdb", "writexl", "xfun", "xml2"))
detach("package:stats", unload = TRUE)
detach("package:utils", unload = TRUE)
library(tidymodels)
library(cli)
install.packages("cli")
install.packages("cli")
library(tidymodels)
gc()
penguins=palmerpenguins
penguins<-library(palmerpenguins)
view(penguins)
load(penguins)
data("palmerpenguins")
library(palmerpenguins)
penguins<-palmerpenguins
summary(penguins_raw)
load(penguins_raw)
print(penguins_raw)
penguins<-penguins_raw
#set a seed on penguins data
set.seed(123)
## 3. split the Palmer Penguins dataset into training and testing sets using a 70/30 split
# first drop na_values from the split
penguins<-drop_na(penguins)
library(tidyverse)
## 3. split the Palmer Penguins dataset into training and testing sets using a 70/30 split
# first drop na_values from the split
penguins<-drop_na(penguins)
penguins<-penguins_raw
## 3. split the Palmer Penguins dataset into training and testing sets using a 70/30 split
#using function initial_split()
resample_split<-initial_split(penguins, prop= .7)
library(tidymodels)
## 3. split the Palmer Penguins dataset into training and testing sets using a 70/30 split
#using function initial_split()
resample_split<-initial_split(penguins, prop= .7)
train_data <- training(resample_split)
test_data <- testing(resample_split)
## 5. Create a 10 fold cross validation dataset based on the training data
penguin_folds <- vfold_cv(train_data, v = 10)
View(penguin_folds)
penguin_folds
penguins<-penguins_raw
## 2.set a seed for reproducible data
set.seed(123)
## 3. split the Palmer Penguins dataset into training and testing sets using a 70/30 split
#using function initial_split()
penguins<-drop_na(penguins)
resample_split<-initial_split(penguins, prop= .7)
## 4. Extract the training and test tibbles into unique objects
train_data <- training(resample_split)
test_data <- testing(resample_split)
## 5. Create a 10 fold cross validation dataset based on the training data
penguin_folds <- vfold_cv(train_data, v = 10)
penguin_folds
library(tidymodels)
library(tidyverse)
library(palmerpenguins)
library(ranger)
penguins<-
na.omit(penguinspenguins::penguins)
penguins<-
na.omit(palmerpenguins::penguins)
set.seed(123)
## 3. split the Palmer Penguins dataset into training and testing sets using a 70/30 split
#using function initial_split()
penguins<-drop_na(penguins)
## 3. split the Palmer Penguins dataset into training and testing sets using a 70/30 split
#using function initial_split()
penguins<-drop_na(penguins)
library(tidymodels)
library(tidymodels)
library(tidyverse)
library(palmerpenguins)
library(ranger)
## 2.set a seed for reproducible data
#load datast and remove na, set seed
penguins<-
na.omit(palmerpenguins::penguins)
set.seed(123)
## 3. split the Palmer Penguins dataset into training and testing sets using a 70/30 split
#using function initial_split()
penguins<-drop_na(penguins)
resample_split<-initial_split(penguins, prop= .7)
## 4. Extract the training and test tibbles into unique objects
train_data <- training(resample_split)
test_data <- testing(resample_split)
## 5. Create a 10 fold cross validation dataset based on the training data
penguin_folds <- vfold_cv(train_data, v = 10)
library(tidymodels)
library(tidyverse)
library(palmerpenguins)
library(ranger)
## 2.set a seed for reproducible data
#load datast and remove na, set seed
penguins<-
na.omit(palmerpenguins::penguins)
set.seed(123)
## 3. split the Palmer Penguins dataset into training and testing sets using a 70/30 split
#using function initial_split()
penguins<-drop_na(penguins)
resample_split<-initial_split(penguins, prop= .7, strata =species) #added strata part b/c Alan said to
## 4. Extract the training and test tibbles into unique objects
train_data <- training(resample_split)
test_data <- testing(resample_split)
## 5. Create a 10 fold cross validation dataset based on the training data
penguin_folds <- vfold_cv(train_data, v = 10)
## Part 2 (daily assignment 16)
multinom_model <-
multinom_reg() %>%
set_engine ("nnet")%>%
set_mode("classification")
rand_forest_model <-
rand_forest() %>%
set_engine("ranger") %>%
set_mode("classification")
penguins_wf_set <-
workflow_set(
preproc =list(species ~ .),
models =list(multinom=
multinom_model, rf =
rand_forest_model)
)
penguins_resample <- penguins_wf_set %>%
workflow_map("fit_resamples",
resamples= penguins_folds, control =
control_resamples(save_pred = TRUE))
library(workflowsets)
penguins_resample <- penguins_wf_set %>%
workflow_map("fit_resamples",
resamples= penguins_folds, control =
control_resamples(save_pred = TRUE))
View(penguin_folds)
penguins_resample <- penguins_wf_set %>%
workflow_map("fit_resamples",
resamples= penguin_folds, control =
control_resamples(save_pred = TRUE))
penguins_res_metrics <-
collect_metrics(penguins_resample)
accuracy_comparison <-
penguins_res_metrics %>%
filter(.metric == "accuracy")
#print comparison
print(accuracy_comparison)
penguins_res_metrics <-
collect_metrics(penguins_resample)
View(accuracy_comparison)
