---
title: "day11-12"
author: "Megan Hoover"
date: "2025-03-11"
format: html
execute:
  echo: true
output:
  html
   self-contained:true
knitr:
  opts_chunk:
    collapse: true
    comment: "#>"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##In Exploratory data Analysis (2016) Peng usefully includes an EDA checklist:
```{r}
penguins<-palmerpenguins::penguins_raw
library(visdat)
library(tidyr)
library(tidyverse)
library(ggpubr)
library(recipes)
library(broom)
library(gridExtra)
```


# 1. Develop a question.
Usually we start EDA with a question that helps focus our interrogation.

Is there a relationship between island location and body size weight a in the palmer penguins dataset?


# 2. Read in your data and check the structure. 
Questions to ask:
What are the dimensions of the data (rows, columns)?
What are the variable types?
Have character variables been coerced to factors? If so, is that appropriate?
Which numeric variables are continuous, which are integers, and which are binary?
```{r}
str(penguins)
sapply(penguins, is.numeric)
```
What are the dimensions of the data? There are 344 rows and 17 columns in this dataset.

What are the variable types? Penguins data is a list that has tbl_df, tbl, and data.frame structure. The variable types include numbers, characters, and dates.

Have character variables been coerced to factors? If so, is that appropriate? No character variables have been coerced to factors in this data set. It could be appropriate depending on the analysis done, for penguin species a number could be given to represent the kind.

# Which numeric variables are continuous (these variables are numeric, but they can take on any value within a range, including decimals ), which are integers(these variables have discrete values with no decimal points), and which are binary (binary variables typically have only two unique values, such as 0 and 1, "yes" and "no", or "TRUE" and "FALSE")?
```{r}
str(penguins)
```
Culmen length, culmen depth, Delta 15N, and and Delta 13 c are continuous data in this tibble.The discrete values are the sample number,flipper length, and body mass.Binary data in this are clutch completion and sex.
 
 
# 3. Summarize the data.Use summary() and table(), vis_dat() and ask the following sorts of questions:
```{r}
summary(penguins)
vis_dat(penguins)

penguins <-penguins %>%
  drop_na()

#range
range(penguins$`Sample Number`)
range(penguins$`Culmen Length (mm)`)
range(penguins$`Culmen Depth (mm)`)
range(penguins$`Flipper Length (mm)`)
range(penguins$`Body Mass (g)`)
range(penguins$`Delta 15 N (o/oo)`)
range(penguins$`Delta 13 C (o/oo)`)

```


# Are there missing observations?

There's missing values/ na values in the clutch completion, comments, Delta 15N and Delta 13c columns,

# What is the range of the numeric variables?

The range of sample numbers: 1-152
The range of culmen length (mm):35.9-58.0
The range of culmen depth (mm): 13.7- 20.0
The range of flipper length (mm):172 -225
The range of body mass(g): 2700 - 5700
The range of delta 15n: 7.99184 - 10.02544
The range of delta delta 13c:-26.84272 - -23.89017

# Do the ranges seem reasonable, or are there values that cause you to worry about data quality? 

These ranges seem normal, except for the delta 13c. There are negative numbers in the data.

# Where are the means and medians of each variable with respect to their minimums and maximums?
```{r}
#mean
summary(penguins$`Sample Number`)       #mean= 51.71 median=39.00
summary(penguins$`Culmen Length (mm)`)  #mean= 44.71 median=44.50
summary(penguins$`Culmen Depth (mm)`)   #mean= 17.65 median=17.90
summary(penguins$`Flipper Length (mm)`) #mean= 196.7 median=195.5
summary(penguins$`Body Mass (g)`)       #mean= 3877  median= 3738
summary(penguins$`Delta 15 N (o/oo)`)   #mean= 9.019 median= 9.041
summary(penguins$`Delta 13 C (o/oo)`)   #mean= -25.33 median=-25.33
```


For factor and character variables: How many observations are there in each level or category?
```{r}
penguins %>%
  count(Species) 

penguins %>%
  count(Island)

penguins %>%
  count(Sex)
```
species:
Species                                       n
  <chr>                                     <int>
1 Adelie Penguin (Pygoscelis adeliae)          13
2 Chinstrap penguin (Pygoscelis antarctica)    14
3 Gentoo penguin (Pygoscelis papua)             7

island:
Island        n
  <chr>     <int>
1 Biscoe        9
2 Dream        17
3 Torgersen     8

sex:
Sex        n
  <chr>  <int>
1 FEMALE    18
2 MALE      16


Table the integer and binary variables: do the counts seem reasonable? If they donâ€™t seem reasonable, then inspect the questionable rows.
```{r}
#The integer column is flipper_length_mm, and the binary variable is sex

table(penguins$`Flipper Length (mm)`)

# Table the binary variable: sex
table(penguins$Sex)

```
These counts seem reasonable in the table.

# 4:Look at the top and the bottom of your data using head() and tail(). 
This will give you a sense of the structure of the data and the type of variables you are dealing with.
```{r}
# View the first few rows (top of the dataset)
head(penguins)

# View the last few rows (bottom of the dataset)
tail(penguins)

```

Be alert to structure and possible data problems or anomalies.
Use skimr::skim() to get a quick overview of the data structure and missing values.
```{r}
skimr::skim(penguins)
```


# 5. Try to answer your question using descriptive measures.
These can include evaluating distributions, central tendency and variation.
```{r}
# Summary statistics of body mass by island
penguins_summary <- penguins %>%
  group_by(Island) %>%
  summarise(
    mean_weight = mean(`Body Mass (g)`, na.rm = TRUE),
    median_weight = median(`Body Mass (g)`, na.rm = TRUE),
    sd_weight = sd(`Body Mass (g)`, na.rm = TRUE),
    min_weight = min(`Body Mass (g)`, na.rm = TRUE),
    max_weight = max(`Body Mass (g)`, na.rm = TRUE)
  )

# View summary statistics
print(penguins_summary)

#A higher standard deviation indicates more variability, and a lower standard deviation suggests that most values are closer to the mean.

#median is a measure of central tendency that is less sensitive to outliers or skewed data than the mean.

#Visualization
# Histogram of body mass by island
ggplot(penguins, aes(x = `Body Mass (g)`, fill = Island)) +
  geom_histogram(binwidth = 100, position = "dodge", alpha = 0.7) +
  labs(title = "Body Mass Distribution by Island",
       x = "Body Mass (grams)", y = "Frequency") +
  facet_wrap(~Island) +
  theme_minimal()
```
Penguins on Biscoe island have the highest average (mean) and median body mass, followed by those on Torgersen, and then Dream island. This central tendency suggests a relationship between island and body mass. The highest variability in body size is in Biscoe penguins, but even with higher variability they have bigger body sizes on Biscoe island.

## Daily assignment 12

Part 1: Normality Testing
Load the airquality dataset in R. What does this dataset represent? Explore its structure using functions like str() and summary().
```{r}
airquality<-airquality
?airquality
str(airquality)
summary(airquality)
```
The air quality data set represents daily air quality measurements during May to September, 1973, in New York.

Perform a Shapiro-Wilk normality test on the following variables: Ozone, Temp, Solar.R, and Wind.
```{r}
# Perform Shapiro-Wilk normality test
shapiro.test(airquality$Ozone)
shapiro.test(airquality$Temp)
shapiro.test(airquality$Solar.R)
shapiro.test(airquality$Wind)

```


# What is the purpose of the Shapiro-Wilk test?

It's a statistical test to check whether the data follows a normal distribution.


# What are the null and alternative hypotheses for this test?

The null hypothesis Ho: The data follows a normal distribution.
The alternative hypothesis H1: The data doesn't follow a normal distribution


# Interpret the p-values. Are these variables normally distributed?

If the p-value is greater than 0.05: We fail to reject the null hypothesis, meaning that the data is not significantly different from a normal distribution.
If the p-value is less than or equal to 0.05: We reject the null hypothesis, meaning that the data is significantly different from a normal distribution, suggesting that the data isn't normally distributed.

Shapiro-Wilk normality test
data:  airquality$Ozone
W = 0.87867, p-value = 2.79e-08

The Ozone data variables are normally distributed.


	Shapiro-Wilk normality test
data:  airquality$Temp
W = 0.97617, p-value = 0.009319

Temp data variables are not normally distributed.


	Shapiro-Wilk normality test
data:  airquality$Solar.R
W = 0.94183, p-value = 9.492e-06

Solar. R variables are normally distributed.


	Shapiro-Wilk normality test
data:  airquality$Wind
W = 0.98575, p-value = 0.1178

Wind variables aren't normally distributed.

# Part 2: Data Transformation and Feature Engineering
Create a new column with case_when tranlating the Months into four seasons (Winter (Nov, Dec, Jan), Spring (Feb, Mar, Apr), Summer (May, Jun, Jul), and Fall (Aug, Sep, Oct)).
```{r}
# don't remove na values yet
airquality_seasons <- airquality 

# Create a new column 'Season' based on the 'Month' column
# case_when(): This function allows you to assign values to a new column based on conditions. 
airquality_seasons <- airquality_seasons %>%
  mutate(season = case_when(
    Month == 11 | Month == 12 | Month == 1 ~ "Winter",
    Month == 2 | Month == 3 | Month == 4 ~ "Spring",
    Month == 5 | Month == 6 | Month == 7 ~ "Summer",
    Month == 8 | Month == 9 | Month == 10 ~ "Fall",
    TRUE ~ "Unknown"  # Handling any missing values (NA) if necessary
  ))
```


Use table to figure out how many observations we have from each season.
```{r}
# Count the number of observations for each season
table(airquality_seasons$season)

```


# Part 3: Data Preprocessing
Normalize the predictor variables (Temp, Solar.R, Wind, and Season) using a recipe
```{r}
#  recipe to normalize the variables Temp, Solar.R, and Wind
recipe_normalize <- recipe(~ Temp + Solar.R + Wind + season, data = airquality_seasons) %>%
  # Impute missing values with the mean for numeric columns
  step_impute_mean(all_numeric(), -all_outcomes()) %>%
  # Normalize the numeric predictors variables (Temp, Solar.R, Wind) to have a mean of 0 and a standard deviation of 1.
  step_normalize(all_numeric(), -all_outcomes())


# Preview the recipe
recipe_normalize

```


# What is the purpose of normalizing data?

To make sure all variables used for predictors equally contribute, without one being over scaled.

# What function can be used to impute missing values with the mean?

the function step_impute can be used to impute missing numeric values.

# prep and bake the data to generate a processed dataset.
```{r}
# Prepare the recipe (estimate means and scaling parameters)
recipe_prep <- prep(recipe_normalize, training = airquality_seasons)

# Apply the transformations to the data (bake)
airquality_processed <- bake(recipe_prep, new_data = airquality_seasons)

# View the processed dataset
head(airquality_processed)
```


# Why is it necessary to both prep() and bake() the recipe?

Prep prepares the data for the transformation by estimating parameters with the training data, while bake applies the transformation on the data. 

# Part 4: Building a Linear Regression Model
Fit a linear model using Ozone as the response variable and all other variables as predictors. Remember that the . notation can we used to include all variables.
```{r}
# Fit the linear regression model. lm() function to fit the model, and the . notation to include all predictor variables automatically.
airquality_lm <- lm(Ozone ~ ., data = airquality_seasons) 

  summary(airquality_lm)
```


# Interpret the model summary output (coefficients, R-squared, p-values) in plain language

The coefficients represent the change in the Ozone level for each one-unit increase in a paired predictor, holding the other predictors constant.The R-squared is interpreted as a percentage that can explain the variability of the model. P values in this tell whether or not a predictor variable is significantly contributing to the model; p-values less than .05 mean that the predictor variable is statistically significant. The variables in this model are mainly not statistically significant.

# Part 5: Model Diagnostics
Use broom::augment to suppliment the normalized data.frame with the fitted values and residuals.
```{r}
# broom::augment() function can be used to add the fitted values and residuals to your data

#remove the na values
airquality_nona <- na.omit(airquality_seasons)

# Augment the data with fitted values and residuals
augmented_data <- augment(airquality_lm, data = airquality_nona)

# View the first few rows of the augmented data
head(augmented_data)
```


# Extract the residuals and visualize their distribution as a histogram and qqplot.
```{r}
# Histogram of residuals
hist_plot <- ggplot(augmented_data, aes(x = .resid)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Histogram of Residuals", x = "Residuals", y = "Frequency") +
  theme_minimal()

# Q-Q plot of residuals
qq_plot <- ggplot(augmented_data, aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line() +
  labs(title = "Q-Q Plot of Residuals") +
  theme_minimal()
```


# Use ggarange to plot this as one image and interpret what you see in them.
```{r}
# Use ggarrange to combine the two plots into one image
grid.arrange(hist_plot, qq_plot, ncol = 2)
```


# Create a scatter plot of actual vs. predicted values using ggpubr with the following setting:

ggscatter(a, x = "Ozone", y = ".fitted",
          add = "reg.line", conf.int = TRUE,
          cor.coef = TRUE, cor.method = "spearman",
          ellipse = TRUE)
```{r}
# Scatter plot of Actual vs. Predicted Ozone
ggscatter(augmented_data, x = "Ozone", y = ".fitted",
          add = "reg.line", conf.int = TRUE,
          cor.coef = TRUE, cor.method = "spearman",
          ellipse = TRUE) +
  labs(title = "Actual vs. Predicted Ozone") +
  theme_minimal()
```
          

# How strong of a model do you think this is?

This appears to be a strong model with a somewhat linear trend. There's a random scatter of points around the regression line suggesting that the model is a good fit for the data.

Render your document to HTML and submit to Canvas. Be sure to include this in your document yml:

format: 
  html:
    self-contained: true